"""
ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë„êµ¬
ì¸ë±ìŠ¤ ìµœì í™”, ì¿¼ë¦¬ ë¶„ì„, ì„±ëŠ¥ íŠœë‹ì„ ìœ„í•œ ë„êµ¬ë“¤
"""
import logging
from typing import Dict, List, Optional, Tuple
from sqlalchemy import text, inspect, Index, create_engine
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError
from datetime import datetime, timedelta

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseOptimizer:
    """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë„êµ¬"""
    
    def __init__(self, db):
        self.db = db
        self.engine = db.engine
        self.inspector = inspect(self.engine)
        
        # ìµœì í™” ê¶Œì¥ì‚¬í•­
        self.optimization_recommendations = {
            'missing_indexes': [],
            'unused_indexes': [],
            'slow_queries': [],
            'table_optimizations': []
        }
    
    def analyze_database_structure(self) -> Dict[str, any]:
        """ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° ë¶„ì„"""
        try:
            analysis = {
                'tables': {},
                'total_tables': 0,
                'total_indexes': 0,
                'total_foreign_keys': 0,
                'timestamp': datetime.now().isoformat()
            }
            
            # í…Œì´ë¸” ì •ë³´ ìˆ˜ì§‘
            for table_name in self.inspector.get_table_names():
                table_info = {
                    'name': table_name,
                    'columns': len(self.inspector.get_columns(table_name)),
                    'indexes': len(self.inspector.get_indexes(table_name)),
                    'foreign_keys': len(self.inspector.get_foreign_keys(table_name)),
                    'primary_keys': len(self.inspector.get_pk_constraint(table_name)['constrained_columns']),
                    'estimated_rows': self._estimate_table_rows(table_name)
                }
                
                analysis['tables'][table_name] = table_info
                analysis['total_tables'] += 1
                analysis['total_indexes'] += table_info['indexes']
                analysis['total_foreign_keys'] += table_info['foreign_keys']
            
            logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ: {analysis['total_tables']}ê°œ í…Œì´ë¸”")
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def analyze_query_performance(self, query: str, params: Dict = None) -> Dict[str, any]:
        """ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„"""
        try:
            start_time = datetime.now()
            
            # ì‹¤í–‰ ê³„íš ë¶„ì„
            explain_query = f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}"
            with self.engine.connect() as conn:
                result = conn.execute(text(explain_query), params or {})
                execution_plan = result.fetchone()
            
            # ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰
            with self.engine.connect() as conn:
                result = conn.execute(text(query), params or {})
                rows = result.fetchall()
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            analysis = {
                'query': query,
                'execution_time': execution_time,
                'rows_returned': len(rows),
                'execution_plan': execution_plan[0] if execution_plan else None,
                'timestamp': datetime.now().isoformat()
            }
            
            # ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼
            if execution_time > 1.0:  # 1ì´ˆ ì´ìƒ ê±¸ë¦¬ëŠ” ì¿¼ë¦¬
                analysis['performance_issue'] = 'slow_query'
                analysis['recommendation'] = 'ì¸ë±ìŠ¤ ì¶”ê°€ ë˜ëŠ” ì¿¼ë¦¬ ìµœì í™” í•„ìš”'
            elif execution_time > 0.5:  # 0.5ì´ˆ ì´ìƒ ê±¸ë¦¬ëŠ” ì¿¼ë¦¬
                analysis['performance_issue'] = 'moderate_query'
                analysis['recommendation'] = 'ëª¨ë‹ˆí„°ë§ í•„ìš”'
            else:
                analysis['performance_issue'] = 'fast_query'
                analysis['recommendation'] = 'ì„±ëŠ¥ ì–‘í˜¸'
            
            logger.info(f"âœ… ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ: {execution_time:.3f}ì´ˆ")
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def find_missing_indexes(self) -> List[Dict[str, any]]:
        """ëˆ„ë½ëœ ì¸ë±ìŠ¤ ì°¾ê¸°"""
        try:
            missing_indexes = []
            
            # ìì£¼ ì¡°íšŒë˜ëŠ” ì»¬ëŸ¼ë“¤ì— ëŒ€í•œ ì¸ë±ìŠ¤ í•„ìš”ì„± ë¶„ì„
            common_query_patterns = [
                # ì‚¬ìš©ì ê´€ë ¨
                {'table': 'users', 'columns': ['employee_id'], 'type': 'unique'},
                {'table': 'users', 'columns': ['email'], 'type': 'unique'},
                {'table': 'users', 'columns': ['department'], 'type': 'index'},
                
                # íŒŒí‹° ê´€ë ¨
                {'table': 'parties', 'columns': ['host_employee_id'], 'type': 'index'},
                {'table': 'parties', 'columns': ['party_date'], 'type': 'index'},
                {'table': 'parties', 'columns': ['restaurant_name'], 'type': 'index'},
                {'table': 'parties', 'columns': ['status'], 'type': 'index'},
                
                # ì¼ì • ê´€ë ¨
                {'table': 'schedules', 'columns': ['employee_id'], 'type': 'index'},
                {'table': 'schedules', 'columns': ['schedule_date'], 'type': 'index'},
                {'table': 'schedules', 'columns': ['schedule_type'], 'type': 'index'},
                
                # ì°¸ì—¬ì ê´€ë ¨
                {'table': 'party_participants', 'columns': ['party_id'], 'type': 'index'},
                {'table': 'party_participants', 'columns': ['employee_id'], 'type': 'index'},
                
                # ëŒ“ê¸€ ê´€ë ¨
                {'table': 'comments', 'columns': ['party_id'], 'type': 'index'},
                {'table': 'comments', 'columns': ['employee_id'], 'type': 'index'},
                {'table': 'comments', 'columns': ['created_at'], 'type': 'index'}
            ]
            
            for pattern in common_query_patterns:
                table_name = pattern['table']
                columns = pattern['columns']
                index_type = pattern['type']
                
                # í…Œì´ë¸”ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if table_name not in self.inspector.get_table_names():
                    continue
                
                # ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
                existing_indexes = self.inspector.get_indexes(table_name)
                index_name = f"idx_{table_name}_{'_'.join(columns)}"
                
                # ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                index_exists = any(
                    idx['name'] == index_name or 
                    set(idx['column_names']) == set(columns)
                    for idx in existing_indexes
                )
                
                if not index_exists:
                    missing_indexes.append({
                        'table': table_name,
                        'columns': columns,
                        'type': index_type,
                        'suggested_name': index_name,
                        'priority': 'high' if 'id' in columns[0] else 'medium',
                        'reason': f"{', '.join(columns)} ì»¬ëŸ¼ì— ëŒ€í•œ {index_type} ì¸ë±ìŠ¤ í•„ìš”"
                    })
            
            logger.info(f"âœ… ëˆ„ë½ëœ ì¸ë±ìŠ¤ ë¶„ì„ ì™„ë£Œ: {len(missing_indexes)}ê°œ ë°œê²¬")
            return missing_indexes
            
        except Exception as e:
            logger.error(f"âŒ ëˆ„ë½ëœ ì¸ë±ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def find_unused_indexes(self) -> List[Dict[str, any]]:
        """ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°"""
        try:
            unused_indexes = []
            
            # PostgreSQLì˜ pg_stat_user_indexes ë·°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ í™•ì¸
            if 'postgresql' in str(self.engine.url):
                query = """
                SELECT 
                    schemaname,
                    tablename,
                    indexname,
                    idx_scan as index_scans,
                    idx_tup_read as tuples_read,
                    idx_tup_fetch as tuples_fetched
                FROM pg_stat_user_indexes 
                WHERE idx_scan = 0 
                AND indexname NOT LIKE '%_pkey'
                ORDER BY tablename, indexname
                """
                
                with self.engine.connect() as conn:
                    result = conn.execute(text(query))
                    for row in result:
                        unused_indexes.append({
                            'table': row.tablename,
                            'index': row.indexname,
                            'scans': row.index_scans,
                            'tuples_read': row.tuples_read,
                            'tuples_fetched': row.tuples_fetched,
                            'recommendation': 'ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ - ì œê±° ê³ ë ¤'
                        })
            
            logger.info(f"âœ… ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ ë¶„ì„ ì™„ë£Œ: {len(unused_indexes)}ê°œ ë°œê²¬")
            return unused_indexes
            
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    def create_recommended_indexes(self) -> Dict[str, any]:
        """ê¶Œì¥ ì¸ë±ìŠ¤ ìƒì„±"""
        try:
            missing_indexes = self.find_missing_indexes()
            created_indexes = []
            failed_indexes = []
            
            for index_info in missing_indexes:
                try:
                    # ì¸ë±ìŠ¤ ìƒì„± SQL
                    if index_info['type'] == 'unique':
                        sql = f"""
                        CREATE UNIQUE INDEX {index_info['suggested_name']} 
                        ON {index_info['table']} ({', '.join(index_info['columns'])})
                        """
                    else:
                        sql = f"""
                        CREATE INDEX {index_info['suggested_name']} 
                        ON {index_info['table']} ({', '.join(index_info['columns'])})
                        """
                    
                    with self.engine.connect() as conn:
                        conn.execute(text(sql))
                        conn.commit()
                    
                    created_indexes.append({
                        'table': index_info['table'],
                        'index_name': index_info['suggested_name'],
                        'columns': index_info['columns'],
                        'type': index_info['type']
                    })
                    
                    logger.info(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì„±ê³µ: {index_info['suggested_name']}")
                    
                except Exception as e:
                    failed_indexes.append({
                        'table': index_info['table'],
                        'index_name': index_info['suggested_name'],
                        'error': str(e)
                    })
                    logger.error(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {index_info['suggested_name']} - {e}")
            
            result = {
                'created_count': len(created_indexes),
                'failed_count': len(failed_indexes),
                'created_indexes': created_indexes,
                'failed_indexes': failed_indexes,
                'timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(created_indexes)}ê°œ ì„±ê³µ, {len(failed_indexes)}ê°œ ì‹¤íŒ¨")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def analyze_table_performance(self) -> Dict[str, any]:
        """í…Œì´ë¸” ì„±ëŠ¥ ë¶„ì„"""
        try:
            table_analysis = {}
            
            for table_name in self.inspector.get_table_names():
                try:
                    # í…Œì´ë¸” í¬ê¸° ë° í†µê³„ ì •ë³´
                    size_query = f"""
                    SELECT 
                        pg_size_pretty(pg_total_relation_size('{table_name}'::regclass)) as total_size,
                        pg_size_pretty(pg_relation_size('{table_name}'::regclass)) as table_size,
                        pg_size_pretty(pg_total_relation_size('{table_name}'::regclass) - 
                                     pg_relation_size('{table_name}'::regclass)) as index_size,
                        (SELECT count(*) FROM {table_name}) as row_count
                    """
                    
                    with self.engine.connect() as conn:
                        result = conn.execute(text(size_query))
                        stats = result.fetchone()
                        
                        if stats:
                            table_analysis[table_name] = {
                                'total_size': stats.total_size,
                                'table_size': stats.table_size,
                                'index_size': stats.index_size,
                                'row_count': stats.row_count,
                                'recommendations': []
                            }
                            
                            # ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­ ìƒì„±
                            if stats.row_count > 10000:
                                table_analysis[table_name]['recommendations'].append(
                                    'ëŒ€ìš©ëŸ‰ í…Œì´ë¸” - íŒŒí‹°ì…”ë‹ ê³ ë ¤'
                                )
                            
                            if stats.index_size and '0 bytes' not in stats.index_size:
                                table_analysis[table_name]['recommendations'].append(
                                    'ì¸ë±ìŠ¤ í¬ê¸° ì ì ˆ'
                                )
                            else:
                                table_analysis[table_name]['recommendations'].append(
                                    'ì¸ë±ìŠ¤ ë¶€ì¡± - ì¶”ê°€ ì¸ë±ìŠ¤ í•„ìš”'
                                )
                
                except Exception as e:
                    logger.warning(f"âš ï¸ í…Œì´ë¸” {table_name} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… í…Œì´ë¸” ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ: {len(table_analysis)}ê°œ í…Œì´ë¸”")
            return table_analysis
            
        except Exception as e:
            logger.error(f"âŒ í…Œì´ë¸” ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def generate_optimization_report(self) -> Dict[str, any]:
        """ìµœì í™” ë³´ê³ ì„œ ìƒì„±"""
        try:
            report = {
                'timestamp': datetime.now().isoformat(),
                'database_structure': self.analyze_database_structure(),
                'missing_indexes': self.find_missing_indexes(),
                'unused_indexes': self.find_unused_indexes(),
                'table_performance': self.analyze_table_performance(),
                'recommendations': [],
                'priority_actions': []
            }
            
            # ì¢…í•© ê¶Œì¥ì‚¬í•­ ìƒì„±
            if report['missing_indexes']:
                report['recommendations'].append({
                    'type': 'index_creation',
                    'priority': 'high',
                    'description': f"{len(report['missing_indexes'])}ê°œì˜ ëˆ„ë½ëœ ì¸ë±ìŠ¤ ìƒì„± í•„ìš”",
                    'action': 'create_recommended_indexes() ì‹¤í–‰'
                })
                report['priority_actions'].append('ëˆ„ë½ëœ ì¸ë±ìŠ¤ ìƒì„±')
            
            if report['unused_indexes']:
                report['recommendations'].append({
                    'type': 'index_cleanup',
                    'priority': 'medium',
                    'description': f"{len(report['unused_indexes'])}ê°œì˜ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ ì œê±° ê³ ë ¤",
                    'action': 'ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ ë¶„ì„ í›„ ì œê±°'
                })
            
            # ì„±ëŠ¥ ê°œì„  ê¸°íšŒ ì‹ë³„
            slow_tables = [
                table for table, info in report['table_performance'].items()
                if 'ëŒ€ìš©ëŸ‰ í…Œì´ë¸”' in info.get('recommendations', [])
            ]
            
            if slow_tables:
                report['recommendations'].append({
                    'type': 'table_optimization',
                    'priority': 'medium',
                    'description': f"{len(slow_tables)}ê°œì˜ ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ìµœì í™” í•„ìš”",
                    'action': 'í…Œì´ë¸” íŒŒí‹°ì…”ë‹ ë˜ëŠ” ì•„ì¹´ì´ë¹™ ê³ ë ¤'
                })
            
            logger.info(f"âœ… ìµœì í™” ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {len(report['recommendations'])}ê°œ ê¶Œì¥ì‚¬í•­")
            return report
            
        except Exception as e:
            logger.error(f"âŒ ìµœì í™” ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _estimate_table_rows(self, table_name: str) -> int:
        """í…Œì´ë¸” í–‰ ìˆ˜ ì¶”ì •"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                return result.scalar() or 0
        except:
            return 0

# ê°œë°œ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
if __name__ == '__main__':
    print("ğŸ§ª ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë„êµ¬ í…ŒìŠ¤íŠ¸")
    
    # Mock ê°ì²´ë¡œ í…ŒìŠ¤íŠ¸
    class MockDB:
        pass
    
    class MockEngine:
        pass
    
    class MockInspector:
        def get_table_names(self):
            return ['users', 'parties', 'schedules']
        
        def get_columns(self, table_name):
            return [{'name': 'id'}, {'name': 'name'}]
        
        def get_indexes(self, table_name):
            return []
        
        def get_foreign_keys(self, table_name):
            return []
        
        def get_pk_constraint(self, table_name):
            return {'constrained_columns': ['id']}
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    mock_db = MockDB()
    mock_db.engine = MockEngine()
    mock_db.engine.url = 'postgresql://test'
    
    optimizer = DatabaseOptimizer(mock_db)
    optimizer.inspector = MockInspector()
    
    # êµ¬ì¡° ë¶„ì„ í…ŒìŠ¤íŠ¸
    structure_analysis = optimizer.analyze_database_structure()
    print(f"ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° ë¶„ì„: {structure_analysis}")
    
    # ëˆ„ë½ëœ ì¸ë±ìŠ¤ ë¶„ì„ í…ŒìŠ¤íŠ¸
    missing_indexes = optimizer.find_missing_indexes()
    print(f"ëˆ„ë½ëœ ì¸ë±ìŠ¤: {len(missing_indexes)}ê°œ")
    
    # ìµœì í™” ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸
    optimization_report = optimizer.generate_optimization_report()
    print(f"ìµœì í™” ë³´ê³ ì„œ: {len(optimization_report.get('recommendations', []))}ê°œ ê¶Œì¥ì‚¬í•­")
