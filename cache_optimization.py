#!/usr/bin/env python3
"""
ìºì‹œ ì‹œìŠ¤í…œ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸
Redis ìºì‹œ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ìµœì í™”í•©ë‹ˆë‹¤.
"""

import sys
import os
import time
import json
from datetime import datetime, timedelta

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from cache_manager import cache_manager
from extensions import db
from app import app

class CacheOptimizer:
    def __init__(self):
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0,
            'errors': 0
        }
        self.performance_data = []
    
    def test_cache_connection(self):
        """ìºì‹œ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        print("ğŸ”— ìºì‹œ ì—°ê²° í…ŒìŠ¤íŠ¸...")
        
        if cache_manager.redis_client:
            try:
                # ì—°ê²° í…ŒìŠ¤íŠ¸
                start_time = time.time()
                cache_manager.redis_client.ping()
                response_time = (time.time() - start_time) * 1000
                
                print(f"   âœ… Redis ì—°ê²° ì„±ê³µ (ì‘ë‹µì‹œê°„: {response_time:.1f}ms)")
                
                # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
                info = cache_manager.redis_client.info()
                print(f"   ğŸ“Š Redis ë²„ì „: {info.get('redis_version', 'Unknown')}")
                print(f"   ğŸ’¾ ì‚¬ìš© ë©”ëª¨ë¦¬: {info.get('used_memory_human', 'Unknown')}")
                print(f"   ğŸ”‘ í‚¤ ê°œìˆ˜: {info.get('db0', {}).get('keys', 0)}")
                
                return True
            except Exception as e:
                print(f"   âŒ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
                return False
        else:
            print("   âš ï¸ Redisê°€ ì˜¤í”„ë¼ì¸ ëª¨ë“œì…ë‹ˆë‹¤.")
            return False
    
    def test_cache_performance(self):
        """ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("\nâš¡ ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        if not cache_manager.redis_client:
            print("   âš ï¸ Redisê°€ ì—°ê²°ë˜ì§€ ì•Šì•„ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        test_data = {
            'string': 'test_string_value',
            'number': 12345,
            'list': [1, 2, 3, 4, 5],
            'dict': {'key1': 'value1', 'key2': 'value2'}
        }
        
        # SET ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        set_times = []
        for i in range(100):
            key = f"test_key_{i}"
            start_time = time.time()
            cache_manager.set(key, test_data, ttl=60)
            set_time = (time.time() - start_time) * 1000
            set_times.append(set_time)
            self.cache_stats['sets'] += 1
        
        avg_set_time = sum(set_times) / len(set_times)
        print(f"   ğŸ“ SET í‰ê·  ì‹œê°„: {avg_set_time:.2f}ms")
        
        # GET ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        get_times = []
        for i in range(100):
            key = f"test_key_{i}"
            start_time = time.time()
            result = cache_manager.get(key)
            get_time = (time.time() - start_time) * 1000
            get_times.append(get_time)
            
            if result:
                self.cache_stats['hits'] += 1
            else:
                self.cache_stats['misses'] += 1
        
        avg_get_time = sum(get_times) / len(get_times)
        hit_rate = self.cache_stats['hits'] / (self.cache_stats['hits'] + self.cache_stats['misses']) * 100
        print(f"   ğŸ“– GET í‰ê·  ì‹œê°„: {avg_get_time:.2f}ms")
        print(f"   ğŸ¯ ìºì‹œ íˆíŠ¸ìœ¨: {hit_rate:.1f}%")
        
        # DELETE ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        delete_times = []
        for i in range(100):
            key = f"test_key_{i}"
            start_time = time.time()
            cache_manager.delete(key)
            delete_time = (time.time() - start_time) * 1000
            delete_times.append(delete_time)
            self.cache_stats['deletes'] += 1
        
        avg_delete_time = sum(delete_times) / len(delete_times)
        print(f"   ğŸ—‘ï¸ DELETE í‰ê·  ì‹œê°„: {avg_delete_time:.2f}ms")
        
        # ì„±ëŠ¥ ë°ì´í„° ì €ì¥
        self.performance_data.append({
            'timestamp': datetime.now().isoformat(),
            'avg_set_time': avg_set_time,
            'avg_get_time': avg_get_time,
            'avg_delete_time': avg_delete_time,
            'hit_rate': hit_rate
        })
    
    def analyze_cache_patterns(self):
        """ìºì‹œ ì‚¬ìš© íŒ¨í„´ ë¶„ì„"""
        print("\nğŸ“Š ìºì‹œ ì‚¬ìš© íŒ¨í„´ ë¶„ì„...")
        
        if not cache_manager.redis_client:
            print("   âš ï¸ Redisê°€ ì—°ê²°ë˜ì§€ ì•Šì•„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        try:
            # í‚¤ íŒ¨í„´ ë¶„ì„
            all_keys = cache_manager.redis_client.keys('*')
            key_patterns = {}
            
            for key in all_keys:
                key_str = key.decode('utf-8') if isinstance(key, bytes) else key
                pattern = key_str.split(':')[0] if ':' in key_str else 'root'
                key_patterns[pattern] = key_patterns.get(pattern, 0) + 1
            
            print("   ğŸ”‘ í‚¤ íŒ¨í„´ ë¶„í¬:")
            for pattern, count in sorted(key_patterns.items(), key=lambda x: x[1], reverse=True):
                print(f"      {pattern}: {count}ê°œ")
            
            # TTL ë¶„ì„
            ttl_analysis = {'no_ttl': 0, 'short_ttl': 0, 'medium_ttl': 0, 'long_ttl': 0}
            
            for key in all_keys[:100]:  # ìƒ˜í”Œë§
                ttl = cache_manager.redis_client.ttl(key)
                if ttl == -1:
                    ttl_analysis['no_ttl'] += 1
                elif ttl < 300:  # 5ë¶„ ë¯¸ë§Œ
                    ttl_analysis['short_ttl'] += 1
                elif ttl < 3600:  # 1ì‹œê°„ ë¯¸ë§Œ
                    ttl_analysis['medium_ttl'] += 1
                else:
                    ttl_analysis['long_ttl'] += 1
            
            print("   â° TTL ë¶„í¬ (ìƒ˜í”Œ 100ê°œ):")
            print(f"      TTL ì—†ìŒ: {ttl_analysis['no_ttl']}ê°œ")
            print(f"      ì§§ì€ TTL (<5ë¶„): {ttl_analysis['short_ttl']}ê°œ")
            print(f"      ì¤‘ê°„ TTL (5ë¶„-1ì‹œê°„): {ttl_analysis['medium_ttl']}ê°œ")
            print(f"      ê¸´ TTL (>1ì‹œê°„): {ttl_analysis['long_ttl']}ê°œ")
            
        except Exception as e:
            print(f"   âŒ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.cache_stats['errors'] += 1
    
    def generate_optimization_recommendations(self):
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        print("\nğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­...")
        
        recommendations = []
        
        # íˆíŠ¸ìœ¨ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if self.cache_stats['hits'] + self.cache_stats['misses'] > 0:
            hit_rate = self.cache_stats['hits'] / (self.cache_stats['hits'] + self.cache_stats['misses']) * 100
            
            if hit_rate < 50:
                recommendations.append("ğŸ¯ ìºì‹œ íˆíŠ¸ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. ìºì‹œ í‚¤ ì „ëµì„ ì¬ê²€í† í•˜ì„¸ìš”.")
            elif hit_rate > 90:
                recommendations.append("âœ… ìºì‹œ íˆíŠ¸ìœ¨ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤.")
            else:
                recommendations.append("ğŸ“ˆ ìºì‹œ íˆíŠ¸ìœ¨ì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ì¶”ê°€ ìµœì í™” ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if self.performance_data:
            latest_data = self.performance_data[-1]
            
            if latest_data['avg_get_time'] > 10:
                recommendations.append("âš¡ GET ì‘ë‹µì‹œê°„ì´ ëŠë¦½ë‹ˆë‹¤. Redis ì„¤ì •ì„ ìµœì í™”í•˜ì„¸ìš”.")
            
            if latest_data['avg_set_time'] > 20:
                recommendations.append("ğŸ“ SET ì‘ë‹µì‹œê°„ì´ ëŠë¦½ë‹ˆë‹¤. ë°ì´í„° í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­
        recommendations.extend([
            "ğŸ”§ Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.",
            "â° ì ì ˆí•œ TTLì„ ì„¤ì •í•˜ì—¬ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•˜ì„¸ìš”.",
            "ğŸ—‚ï¸ í‚¤ ë„¤ì´ë° ì»¨ë²¤ì…˜ì„ ì¼ê´€ì„± ìˆê²Œ ìœ ì§€í•˜ì„¸ìš”.",
            "ğŸ“Š ìºì‹œ ì‚¬ìš© íŒ¨í„´ì„ ì •ê¸°ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."
        ])
        
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        return recommendations
    
    def create_cache_config_template(self):
        """ìºì‹œ ì„¤ì • í…œí”Œë¦¿ ìƒì„±"""
        print("\nğŸ“‹ ìºì‹œ ì„¤ì • í…œí”Œë¦¿ ìƒì„±...")
        
        config_template = """# Redis ìºì‹œ ìµœì í™” ì„¤ì •
# .env íŒŒì¼ì— ì¶”ê°€í•˜ì„¸ìš”

# Redis ì—°ê²° ì„¤ì •
REDIS_URL=redis://localhost:6379/0
REDIS_MAX_CONNECTIONS=20
REDIS_SOCKET_TIMEOUT=5
REDIS_SOCKET_CONNECT_TIMEOUT=2

# ìºì‹œ TTL ì„¤ì • (ì´ˆ)
CACHE_DEFAULT_TTL=3600          # ê¸°ë³¸ TTL: 1ì‹œê°„
CACHE_USER_TTL=1800             # ì‚¬ìš©ì ë°ì´í„°: 30ë¶„
CACHE_RESTAURANT_TTL=7200       # ì‹ë‹¹ ë°ì´í„°: 2ì‹œê°„
CACHE_PARTY_TTL=900             # íŒŒí‹° ë°ì´í„°: 15ë¶„
CACHE_SCHEDULE_TTL=600          # ì¼ì • ë°ì´í„°: 10ë¶„

# ìºì‹œ í‚¤ íŒ¨í„´
CACHE_KEY_PREFIX=lunch_app
CACHE_KEY_SEPARATOR=:

# ì„±ëŠ¥ ìµœì í™”
CACHE_COMPRESSION=true
CACHE_SERIALIZATION=json
CACHE_POOL_SIZE=10

# ëª¨ë‹ˆí„°ë§
CACHE_MONITORING=true
CACHE_LOG_LEVEL=INFO
"""
        
        with open('cache_config_template.env', 'w', encoding='utf-8') as f:
            f.write(config_template)
        
        print("âœ… ìºì‹œ ì„¤ì • í…œí”Œë¦¿ì´ cache_config_template.envì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def run_optimization(self):
        """ì „ì²´ ìµœì í™” ì‹¤í–‰"""
        print("ğŸš€ ìºì‹œ ì‹œìŠ¤í…œ ìµœì í™” ì‹œì‘")
        print("=" * 50)
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        cache_connected = self.test_cache_connection()
        
        if cache_connected:
            # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            self.test_cache_performance()
            
            # íŒ¨í„´ ë¶„ì„
            self.analyze_cache_patterns()
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self.generate_optimization_recommendations()
        
        # ì„¤ì • í…œí”Œë¦¿ ìƒì„±
        self.create_cache_config_template()
        
        # ê²°ê³¼ ìš”ì•½
        print("\nğŸ“‹ ìµœì í™” ê²°ê³¼ ìš”ì•½")
        print("=" * 50)
        print(f"ğŸ”— ìºì‹œ ì—°ê²°: {'âœ… ì„±ê³µ' if cache_connected else 'âŒ ì‹¤íŒ¨'}")
        print(f"ğŸ“Š ìºì‹œ í†µê³„:")
        print(f"   â€¢ íˆíŠ¸: {self.cache_stats['hits']}íšŒ")
        print(f"   â€¢ ë¯¸ìŠ¤: {self.cache_stats['misses']}íšŒ")
        print(f"   â€¢ ì„¤ì •: {self.cache_stats['sets']}íšŒ")
        print(f"   â€¢ ì‚­ì œ: {self.cache_stats['deletes']}íšŒ")
        print(f"   â€¢ ì˜¤ë¥˜: {self.cache_stats['errors']}íšŒ")
        
        if self.performance_data:
            latest = self.performance_data[-1]
            print(f"âš¡ ì„±ëŠ¥ ì§€í‘œ:")
            print(f"   â€¢ GET í‰ê· : {latest['avg_get_time']:.2f}ms")
            print(f"   â€¢ SET í‰ê· : {latest['avg_set_time']:.2f}ms")
            print(f"   â€¢ íˆíŠ¸ìœ¨: {latest['hit_rate']:.1f}%")
        
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­: {len(recommendations)}ê°œ ìƒì„±ë¨")
        
        return cache_connected

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    optimizer = CacheOptimizer()
    
    try:
        success = optimizer.run_optimization()
        
        if success:
            print("\nğŸ‰ ìºì‹œ ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return 0
        else:
            print("\nâš ï¸ Redis ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            return 1
            
    except Exception as e:
        print(f"\nâŒ ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
